{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27cc28de-a953-4699-b34b-9f3b2110c0d0",
   "metadata": {},
   "source": [
    "# Sieci neuronowe. Podstawy\n",
    "## 1. Podstawowe pojęcia\n",
    "\n",
    "ML – skrót pojęcia Machine Learning\n",
    "\n",
    "DL – skrót pojęcia Deep Learning\n",
    "\n",
    "model – sieć neuronowa używana do rozwiązania konkretnego przykładu\n",
    "\n",
    "sieć neuronowa – inaczej zwana NN, Neural Network, podstawowy budulec algorytmów Deep Learningowych, który na bazie dostarczonych wejść, uczy się np. klasyfikować obiekty, wyznaczać, gdzie dany obiekt się znajduje itp.\n",
    "\n",
    "warstwa wejściowa – input layer, pierwsza warstwa sieci neuronowej. To do niej docierają dane wejściowe, które są następnie propagowane dalej\n",
    "\n",
    "warstwa wyjściowa – output layer, ostatnia warstwa sieci neuronowej, która dostarcza odpowiednie wyjście, np. wartości prawdopodobieństwa poszczególnych klas (czy dany obiekt to kot, pies czy czołg)\n",
    "\n",
    "hidden layer – wszystkie warstwy pomiędzy warstwą wejściową oraz wyjściową sieci. To tam dzieje się cała magia sieci neuronowej\n",
    "\n",
    "funkcja aktywacyjna – activation function – to dzięki niej nasza sieć nie jest modelem liniowym, który tylko przekazuje dane wejściowe i mnoży je przez odpowiednie współczynniki. Wprowadza nieliniowość do naszego modelu. Bez nieliniowości nie ma sieci neuronowych\n",
    "\n",
    "optymalizator – optimizer – obiekt służący do nauki naszej sieci neuronowej poprzez odpowiednie modyfikowanie wag poszczególnych elementów sieci\n",
    "\n",
    "ML framework – określenie na bibliotekę używaną do stworzenia/nauki sieci neuronowej\n",
    "\n",
    "funkcja błędu – loss function, loss – funkcja mówiąca nam, jak bardzo nasze wartości są niezgodne z oczekiwanymi wartościami. Na bazie jej obliczeń odbywa się korekcja wag przez optimizer\n",
    "\n",
    "forward propagation – przekazywanie danych od wejścia do wyjścia sieci\n",
    "\n",
    "back propagation – propagacja wsteczna, główny mechanizm szkolenia sieci neuronowej. Na bazie wyników loss function przy pomocy odpowiednich metod matematycznych wagi naszej sieci są modyfikowane, przez co osiągany jest efekt uczenia się. Szczegóły matematyczne tej tematyki są poza zakresem tego kursu.\n",
    "\n",
    "Gradient Descent – metoda optymalizacji sieci neuronowych używana przez optimizer.\n",
    "\n",
    "### Podstawowe frameworki do szkolenia sieci neuronowych, ich plusy i minusy\n",
    "\n",
    "Podstawowym językiem programowania w ML i DL jest Python, choć istnieje też wsparcie dla innych języków.\n",
    "\n",
    "R – język stosowany głównie w przypadku statystyki, Big Data oraz Data Science (inne gałęzie ML skupiające się na analizie danych oraz wyciąganiu wniosków)\n",
    "\n",
    "C/C++ – język, w którym większość frameworków ma napisane swoje \"bebechy\". Istnieje możliwość stosowania go do pisania algorytmów ML, lecz jest to uciążliwe\n",
    "\n",
    "Julia – raczkujący język, który cieszy się zainteresowaniem. Łączy prostotę działania Pythona oraz szybkość C/C++ (Python jest bardzo wolny w porównaniu do C/C++). Ponieważ jest to język w fazie rozwoju, nie istnieje zbyt duża baza gotowych bibliotek, a zatem tworzenie bardziej zaawansowanych rzeczy wymaga dobrej znajomości matematyki oraz działania poszczególnych trybików w sieci neuronowej\n",
    "\n",
    "Python – podstawowy język w DL, używany przez zdecydowaną większość programistów ML. Posiada bardzo rozbudowaną bazę dostępnych bibliotek oraz gotowych rozwiązań\n",
    "\n",
    "Frameworki pozwalające na tworzenie, szkolenie oraz używanie sieci neuronowych. Na rynku obecnie są dwa dominujące produkty, obydwa są darmowe:\n",
    "\n",
    "TensorFlow – Potocznie zwany tf, framework stworzony przez Google na potrzeby wewnętrzne. Obecnie w wersji 2.x. Przed jej wprowadzeniem było to bardzo toporne narzędzie, wymagające dokładnej wiedzy, co i jak chcemy zrobić, gdzie przesłać itp. Obecnie framework jest przyjemny w użytkowaniu, jednak przy chęci zrobienia zaawansowanych rzeczy, również trzeba mieć sporą wiedzę i umiejętności. Swój sukces zawdzięcza wchłonięciu innego frameworka – Kerasa. Jest powszechnie stosowany w industrialnych rozwiązaniach oraz poszukiwany przez pracodawców.\n",
    "\n",
    "PyTorch – framework początkowo rozwijany jako 3rd party project, czyli niepowiązany z żadną firmą. Dopiero niedawno został wchłonięty przez Facebooka i od tego momentu jest rozwijany pod jego skrzydłami. Od kilku miesięcy rośnie jego dominacja w badaniach oraz pracach naukowych. Wiąże się to z tym, że jest bardzo prosty w obsłudze, banalnie się w nim prototypuje oraz ma mniejszy próg wejścia niż TensorFlow. Jest jednak rzadziej stosowany w industrialnych rozwiązaniach.\n",
    "\n",
    "pozostałe frameworki to de facto nakładki na dwa powyższe np. Fastai, Darknet.\n",
    "\n",
    "## 2. Budowa sieci neuronowej — część I\n",
    "\n",
    "Sieć neuronowa to kalkulator wykonujący obliczenia na wielowymiarowych macierzach i wektorach.\n",
    "\n",
    "Macierz można rozumieć jako tablicę a wektor jako listę liczb. Obliczenia na macierzach i wektorach sprowadzają się zatem do odpowiednich sekwencji obliczeń na tych liczbach i prowadzą do powstania innych macierzy i wektorów.\n",
    "\n",
    "### Funkcje aktywacyjne\n",
    "\n",
    "Funkcja aktywacyjna wprowadza nieliniowość do naszego modelu, przez co pasuje on do o wiele szerszej klasy problemów. Zasada działania jest następująca:\n",
    "\n",
    "Zmodyfikuj sygnał wejściowy w zależności od jego wartości, wykonując operację matematyczną zależną od danej funkcji aktywacyjnej.\n",
    "\n",
    "Najbardziej powszechne funkcje aktywacyjne to:\n",
    "- sigmoid\n",
    "- sofmtax\n",
    "- relu\n",
    "- tanh\n",
    "- leakyrelu\n",
    "- swish\n",
    "- mish\n",
    "\n",
    "## 3. Budowa sieci neuronowej — część II\n",
    "\n",
    "Dwa sposoby używania optimizerów\n",
    "\n",
    "Sposób 1. Każdy optimizer posiada swoją nazwę, która go w pełni identyfikuje. Aby go użyć z domyślnymi parametrami, wystarczy podczas kompilacji modelu jako optimizer wpisać tę nazwę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0886e0-8e7c-4c35-8103-5b7b3899637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#przykładowy optimizer z tensorflow\n",
    "tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam',\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "# przekazujemy nazwę, która identyfikuje nasz optimizer\n",
    "model.compile(optimizer='adam', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16facd68-cd37-4ccc-a3a3-9c93ed6ee6b4",
   "metadata": {},
   "source": [
    "Sposób 2. Jeżeli chcemy zmienić jakąkolwiek wartość (np. learning_rate), powinniśmy przekazać nowy obiekt optimizera do metody compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53845cb-c5a7-4cd1-a79e-5296cf95dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy obiekt typu Adam z naszym learning rate\n",
    "adam_optim = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# tworzymy model\n",
    "# ...\n",
    "\n",
    "# przekazujemy obiekt zamiast nazwy\n",
    "model.compile(optimizer=adam_optim, ...)\n",
    "\n",
    "# szkolimy model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5807593f-0be7-4afc-aeca-3edae9575428",
   "metadata": {},
   "source": [
    "### Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576e575-daa3-4572-8716-03081f317d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ...,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(wlasna_funkcja)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c847d-ed41-4fef-ab0c-6a7084d616ee",
   "metadata": {},
   "source": [
    "Schedulers\n",
    "\n",
    "Najprostszym sposobem, aby uzyskać zmianę learning_rate jest ustawienie parametru decay w optimizerach (jest to parametr klasy bazowej, więc trzeba go szukać tutaj)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181575d-e893-443e-bdf8-0a5710b0eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = keras.optimizers.Adam(lr=0.01, decay=1e-5)\n",
    "\n",
    "Exponential scheduling ze stałą wartością learning_rate – zmiana wartości learning_rate wykładniczo:\n",
    "\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.05 * 0.2**(epoch / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198a6aa-7351-4ef6-ac7b-59d305b42d9c",
   "metadata": {},
   "source": [
    "Exponential scheduling z wartością zależną od aktualnego learning_rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ca893-42af-4f85-a84d-c91169e546e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, current_lr):\n",
    "   return current_lr * 0.2**(epoch / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b129e-d616-4e85-8b85-9befced05c60",
   "metadata": {},
   "source": [
    "Scheduler zależny od numeru epoki:\n",
    "- dla pierwszych epok ustaw duża wartość\n",
    "- dla epok do 20 ustaw mniejszą wartość\n",
    "- dla pozostałych ustaw małą wartość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b16ca5-8525-4911-84f5-7d00969ea687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.05\n",
    "    elif epoch < 20:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c82e0-7bab-410e-97ae-446d37132fda",
   "metadata": {},
   "source": [
    "### Loss Functions – funkcje błędu\n",
    "\n",
    "Użyta funkcja błędu\n",
    "\n",
    "W TensorFlow są dwie bardzo podobne funkcje błędu:\n",
    "\n",
    "- sparse_categorical_corssentropy\n",
    "\n",
    "- categorical_crossentropy\n",
    "\n",
    "Główna różnica pomiędzy wersjami ze 'sparse' oraz bez jest następująca: w przypadku funkcji błędów ze słówkiem 'sparse', nasze labels, czyli klasy wyjściowe, muszą być liczbą całkowitą: [1, 0, 2].\n",
    "\n",
    "Znaczy to tyle, że:\n",
    "\n",
    "- pierwszy obrazek jest klasy 1\n",
    "- drugi obrazek jest klasy 0\n",
    "- trzeci obrazek jest klasy 2\n",
    "\n",
    "W naszym przypadku labels są w formie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc1f8a-6611-44cc-8928-782acebc64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "\n",
    "# [9 0 0 ... 3 0 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68914886-eb1d-4b7d-aa4d-20dd0036ab43",
   "metadata": {},
   "source": [
    "Zatem mamy podane za pomocą pojedynczej liczby, do jakiej klasy należy dane zdjęcie.\n",
    "\n",
    "W przypadku funkcji błędów bez słówka 'sparse', nasze labels musiałyby być w formie tzw. one-hot encoded, czyli wektora uzupełnionego samymi zerami, z wyjątkiem naszej klasy, która miałaby wartość 1.\n",
    "\n",
    "W przypadku naszej pierwszej próbki:\n",
    "\n",
    "- wersja 'sparse' miałaby wartość 9\n",
    "- wersja bez 'sparse' miałaby wartość [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    "\n",
    "Aby zamienić nasze wyjścia na formę akceptowaną przez metody bez 'sparse' możemy użyć f.keras.utils.to_categorical(), jak na poniższym przykładzie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25710fb-5d31-44f9-bead-e9d1a0fcf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"spase: \", labels[0])\n",
    "one_hot_label = tf.keras.utils.to_categorical(labels[0])\n",
    "print(\"one hot encoded \", one_hot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fce95b-b032-4328-b758-a287cca7565a",
   "metadata": {},
   "source": [
    "Otrzymamy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa5892-ee72-4b23-a754-8f3493f1ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "spase:  9\n",
    "one hot encoded  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c990588-5490-4abd-bcd9-909fa1ceaf4f",
   "metadata": {},
   "source": [
    "Jeżeli chcemy przekonwertować dane z formatu one-hot encoded do 'sparse', należy użyć np.argmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b422c7b-9da7-4250-ba32-f796a24db8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(one_hot_label) # należy pamiętać o axis=1 jeśli konwertujemy całą listę"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22321e-6e0b-4a4b-84a8-4a854cf48735",
   "metadata": {},
   "source": [
    "Stosować 'sparse', czy nie? Nie ma zbyt dużej różnicy w prostych przypadkach. Pojawia się ona, gdy chcemy używać bardziej zaawansowanych funkcji błędu plus np. metody label smoothing, ale jeśli chodzi o zakres tego kursu, nie ma to najmniejszego znaczenia.\n",
    "\n",
    "Warto pamiętać tylko, że zasadniczym powodem stosowania one-hot encoding jest zapewnienie, że numerując, nie wprowadzamy sztucznych powiązań między kategoriami (na przykład porządku)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2e533-cef5-43ab-acae-06c007efc319",
   "metadata": {},
   "source": [
    "\n",
    "Dwa sposoby używania loss functions\n",
    "\n",
    "Sposób 1. Każda funkcja posiada swoją wbudowaną nazwę, którą możemy wpisać, aby uzyskać loss function z domyślną konfiguracją np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee29057-ab34-4cdc-a71e-74a913c75ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0,\n",
    "    reduction=losses_utils.ReductionV2.AUTO,\n",
    "    name='categorical_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838ae27-4901-4244-80b7-d6dbd9638064",
   "metadata": {},
   "source": [
    "#przekazujemy nazwę, która identyfikuje naszą funkcję\n",
    "model.compile(..., loss='categorical_crossentropy)\n",
    "\n",
    "Sposób 2. Jeżeli chcemy zmienić jakąkolwiek wartość parametru (np. from_logits), zamiast nazwy musimy stworzyć obiekt funkcji i przekazać go do naszej metody uczącej zamiast nazwy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600ce62-202b-414e-bcf1-4238b1b61dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy obiekt CategoricalCrossEntropy\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# tworzymy model\n",
    "# ...\n",
    "\n",
    "# przekazujemy obiekt zamiast nazwy\n",
    "model.compile(..., loss=loss_fn)\n",
    "\n",
    "# szkolimy model\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b025e-5335-40e2-806f-b612ea9d7715",
   "metadata": {},
   "source": [
    "Jest to sytuacja analogiczna jak w przypadku optimizerów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972d2f7-037a-44be-b391-a5b51a965bd1",
   "metadata": {},
   "source": [
    "## 4. Podstawowe warstwy sieci neuronowych\n",
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21329e-0d17-42c3-8103-c8278b564441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(32)\n",
    "\n",
    "# albo bardziej dosłownie\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(units=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f57388-fe42-4c3f-beb6-848d570e098f",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d68c3-00f4-4b6e-a27c-d599f692d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = tf.keras.layers.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606024d-343e-4651-a859-281b707a7eae",
   "metadata": {},
   "source": [
    "### Tworzenie sieci neuronowej w TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9235e5a-6a1c-483a-8e36-79266ad04d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nasz_model(dane_wejsciowe)\n",
    "\n",
    "input_data = tf.ones((16, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1655d5-0f20-4f2f-beae-026431a724a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "#stworzenie modelu\n",
    "\n",
    "# sposób pierwszy\n",
    "\n",
    "seq_model = tf.keras.Sequential()\n",
    "\n",
    "# input_shape jest niewymagane, lecz pozwala powiedzieć,\n",
    "# \"chcę mieć taki rozmiar danych wejściowych\", co pozwala\n",
    "# uniknąć głupich pomyłek w stylu: przekazujemy inny rozmiar,\n",
    "# bo zapomnieliśmy np. zmniejszyć obrazów wejściowych\n",
    "\n",
    "seq_model.add(layers.Flatten(input_shape=[3, 3]))\n",
    "seq_model.add(layers.Dense(16, name=\"input_layer\"))\n",
    "seq_model.add(layers.Dense(32, name=\"hidden_layer\"))\n",
    "seq_model.add(layers.Dense(4, name=\"output_layer\"))\n",
    "\n",
    "#sposób drugi\n",
    "\n",
    "seq_model_2 = tf.keras.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, name=\"input_layer\"),\n",
    "    layers.Dense(32, name=\"hidden_layer\"),\n",
    "    layers.Dense(4, name=\"output_layer\")\n",
    "])\n",
    "\n",
    "# uruchomienie naszego modelu z wygenerowanymi danymi\n",
    "output = seq_model_2(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dba7ee-78c1-470c-8f2a-a5fa8d63ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# wydobycie obrazów oraz labelek\n",
    "images, labels = train\n",
    "\n",
    "# normalizacja wartości pikseli (maks. wartość\n",
    "# wynosi 255.0, czyli aby znormalizować nasze dane,\n",
    "# musimy podzielić każdy piksel przez maks. wartość)\n",
    "images = images/255.0\n",
    "\n",
    "# zapisujemy dane jako int\n",
    "labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8d13-0cea-4220-b3c5-c0cff5971cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(list_pythonowa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de1bd6-695f-4080-b1dc-8f93106d982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(images, labels, test_size=0.1)\n",
    "\n",
    "# stworzenie zbioru typu Dataset z naszej listy\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "# ustawienie batch_size na 32 oraz przetasowanie na bazie 1000 próbek\n",
    "train_ds = train_ds.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247104c-3330-4c52-89fa-76b72de6b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mnist_model = tf.keras.Sequential([\n",
    "    # spłaszczanie obrazka do wektora jednowymiarowego\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dense(150, activation='relu'),\n",
    "\n",
    "    # ostatnia warstwa posiada tyle neuronów ile mamy klas\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c28b5-4ed9-4b59-8670-054ffe39fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=[24, 24]),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c7aee-c40d-4d1b-9d8c-7dd417ec78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f542993-f483-46d3-835e-1df84f2d7b2b",
   "metadata": {},
   "source": [
    "### Kompilacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924280e-36a8-445d-bf46-2627d69ded15",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mnist_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1fd53-0eef-42d6-821e-f1b690afb0a1",
   "metadata": {},
   "source": [
    "### Szkolenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbd531-1b87-424c-84d4-7c13cdd70c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = f_mnist_model.fit(train_ds, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84843a12-196d-4976-ae65-78457ea18f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(train_stats.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec6faa-1c7d-406e-bc94-c17f84fc97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = f_mnist_model.predict(X_test)\n",
    "print(\"probs : \", y_pred[2])\n",
    "print(\"klasa :\", np.argmax(y_pred[2]))\n",
    "print(\"rzeczywista klasa: \", y_test[2])\n",
    "\n",
    "\"\"\"\n",
    "probs :  [4.0548810e-15 1.0000000e+00 9.3477974e-17 5.3090128e-13 7.5702587e-15\n",
    " 5.9295928e-25 2.1536054e-11 3.4459677e-24 2.8725664e-16 2.4974258e-22]\n",
    "klasa : 1\n",
    "rzeczywista klasa:  1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f1d91-0179-45fe-9ab8-9c31e3fe2455",
   "metadata": {},
   "source": [
    "### Functional API do stworzenia modelu w TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1a2d5-475f-437a-8e01-64e45b261bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworzenie wejścia\n",
    "\n",
    "input = tf.keras.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# możemy wypisać, co ta warstwa przyjmuje - jest to rozmiar\n",
    "# naszego obrazka bez batch_size (który ma wartość None)\n",
    "print(input)\n",
    "\n",
    "# spłaszczenie wejścia\n",
    "input_flat = layers.Flatten(input_shape=[28,28])(input)\n",
    "\n",
    "# nasza kolejna warstwa jest typu Dense, jak poprzednio, ale od razu\n",
    "# i bezpośrednio przekazujemy jej wejście, tak jak funkcji w Pythonie:\n",
    "hidden_1 =layers.Dense(320, activation='relu', name=\"hidden_1\")(input_flat)\n",
    "hidden_2 =layers.Dense(150, activation='relu', name=\"hidden_2\")(hidden_1)\n",
    "\n",
    "# złączamy wyniki z obu warstw za pomocą warstwy typu Concatenate\n",
    "concat_layer = layers.Concatenate()([input_flat, hidden_2])\n",
    "output = layers.Dense(10, activation='softmax')(concat_layer)\n",
    "\n",
    "# tworzymy model, przekazując mu co ma być naszymi wyjściami, a co wejściami\n",
    "model_res = tf.keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "# podsumowanie naszego modelu\n",
    "model_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f66a68-aa52-497a-9e63-255f32ecb95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pamiętajmy, aby nie dodawać pierwszego wymiaru (batch_size)\n",
    "text = np.array([[\"ala ma kota\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40759d81-6c3b-4294-852d-df62caa4c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array([[\"ala ma kota\"]])\n",
    "\n",
    "input_1 = tf.keras.Input(shape=text.shape[1:])\n",
    "input_2 = tf.keras.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# nasza kolejna warstwa jest typu Dense, jak poprzednio,\n",
    "# ale od razu przekazujemy jej wejście, tak jak funkcji w Pythonie\n",
    "hidden_1 =layers.Dense(320, activation='relu')(input_1)\n",
    "hidden_2 =layers.Dense(150, activation='relu')(hidden_1)\n",
    "\n",
    "# złączamy wyniki naszych warstw za pomocą warstwy\n",
    "# typu Concatenate podając jako argumenty input_1 oraz hidden_2\n",
    "concat_layer = layers.Concatenate()([input_1, hidden_2])\n",
    "output = layers.Dense(10, activation='softmax')(concat_layer)\n",
    "\n",
    "# tworzymy model, przekazując mu co ma być naszymi wyjściami, a co wejściami\n",
    "model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output])\n",
    "\n",
    "# podsumowanie naszego modelu\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6cd72-c630-406e-88f9-16894e739191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kompilacja\n",
    "model_res.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#szkolenie na takich samych danych jak poprzednio\n",
    "train_stats = model_res.fit(train_ds, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026dff8d-6ba6-4d1f-9572-ab02df28789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
